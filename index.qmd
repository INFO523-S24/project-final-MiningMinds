---
title: "Project Title"
subtitle: "INFO 523 - Project Final"
author: 
  - name: Roxana Akbarsharifi<br>Omid Zandi<br>Deema Albluwi<br>Gowtham Gopalakrishnan<br>Nandhini Anne<br>Sai Navya Reddy Busireddy
    affiliations:
      - name: "School of Information, University of Arizona"
description: "Project description"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract

In this project, we aimed to apply various methods and their combinations to a complex and highly imbalanced classification problem. Initially, we constructed a balanced subset of our data, containing an equal number of fraud and non-fraud transactions using the "Random Majority Under Sampling Technique". This approach aids our models in better recognizing patterns indicative of fraudulent activities. A balanced subsample in our context refers to a dataset with an equal proportion of fraud and non-fraud transactions, achieving a 50/50 ratio. Subsequently, we trained three machine learning algorithms: Random Forest, K-Nearest Neighbors, and XGBoost, and combined them using a stacked generalization approach based on the balanced dataset. We utilized a grid search method to hyper-tune the machine learning algorithms. More precisely, we evaluated the enhancement in performance of the three machine learning models achieved by integrating various models through a novel ensemble method known as stacked generalization, using logistic regression as the meta-learner. After evaluating the performance of the models based on the balanced dataset, we applied the models to the entire dataset. The main findings of this research are twofold: 1. The trained models exhibit high generalizability. 2. The stacked model performs as well as the best base learner.