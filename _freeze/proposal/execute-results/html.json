{
  "hash": "934a3af3c1926f467bb0c67fd8385f74",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Fraud Detection using Ensemble Learning\nsubtitle: Proposal\nauthor:\n  - name: Roxana Akbarsharifi<br>Omid Zandi<br>Deema Albluwi<br>Gowtham Gopalakrishnan<br>Nandhini Anne<br>Sai Navya Reddy Busireddy\n    affiliations:\n      - name: 'School of Information, University of Arizona'\ndescription: Credit Card Fraud Detection Using Anomaly Detection Techniques\nformat:\n  html:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    embed-resources: true\neditor: visual\ncode-annotations: hover\nexecute:\n  warning: false\n---\n\n## High Level Goal\n\nThe overarching goal is to evaluate the enhancement in machine learning models’ performance achieved by integrating various models through a novel ensemble method known as stacked generalization. To this end, we will apply the stacked model to a comlex classification problem aimed at detecting fraudulent credit card transactions.\n\n## Project Description and Motivation\n\nOur project is motivated by the goal of accurately identifying fraudulent transactions using ensemble learning approaches (baggin, boosting, and stacking). We plan to use advanced data mining and machine learning techniques to create a model capable of spotting transactions that stand out as potentially fraudulent. A key part of our project involves comparing different machine learning models to see which one performs best on each dataset we study. We're committed to understanding why certain models are more effective in specific situations. This approach will not only help us find the most effective way to detect fraud but also deepen our understanding of how these models work. Through this work, we aim to make a meaningful contribution to both the field of study and the practical efforts to secure financial transactions.\n\n## Dataset\n\nWe chose a dataset comprising over 550,000 credit card transactions made by European cardholders in 2023. This dataset is particularly appealing because of its substantial size, the anonymity of its features, and its real-world relevance. It includes 31 features: a unique transaction ID, 28 anonymized attributes (V1-V28) that could encompass various transaction details such as time, location, and merchant category, the transaction amount, and a binary class indicating whether the transaction was fraudulent. The data's anonymization ensures privacy and ethical compliance, while the binary classification makes it suitable for supervised learning approaches in fraud detection.\n\n::: {#load-pkgs .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n```\n:::\n\n\n::: {#cell-load-dataset .cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('./data/creditcard_2023.csv')\ndf.head()\n```\n\n::: {#load-dataset .cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-0.260648</td>\n      <td>-0.469648</td>\n      <td>2.496266</td>\n      <td>-0.083724</td>\n      <td>0.129681</td>\n      <td>0.732898</td>\n      <td>0.519014</td>\n      <td>-0.130006</td>\n      <td>0.727159</td>\n      <td>...</td>\n      <td>-0.110552</td>\n      <td>0.217606</td>\n      <td>-0.134794</td>\n      <td>0.165959</td>\n      <td>0.126280</td>\n      <td>-0.434824</td>\n      <td>-0.081230</td>\n      <td>-0.151045</td>\n      <td>17982.10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.985100</td>\n      <td>-0.356045</td>\n      <td>0.558056</td>\n      <td>-0.429654</td>\n      <td>0.277140</td>\n      <td>0.428605</td>\n      <td>0.406466</td>\n      <td>-0.133118</td>\n      <td>0.347452</td>\n      <td>...</td>\n      <td>-0.194936</td>\n      <td>-0.605761</td>\n      <td>0.079469</td>\n      <td>-0.577395</td>\n      <td>0.190090</td>\n      <td>0.296503</td>\n      <td>-0.248052</td>\n      <td>-0.064512</td>\n      <td>6531.37</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-0.260272</td>\n      <td>-0.949385</td>\n      <td>1.728538</td>\n      <td>-0.457986</td>\n      <td>0.074062</td>\n      <td>1.419481</td>\n      <td>0.743511</td>\n      <td>-0.095576</td>\n      <td>-0.261297</td>\n      <td>...</td>\n      <td>-0.005020</td>\n      <td>0.702906</td>\n      <td>0.945045</td>\n      <td>-1.154666</td>\n      <td>-0.605564</td>\n      <td>-0.312895</td>\n      <td>-0.300258</td>\n      <td>-0.244718</td>\n      <td>2513.54</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-0.152152</td>\n      <td>-0.508959</td>\n      <td>1.746840</td>\n      <td>-1.090178</td>\n      <td>0.249486</td>\n      <td>1.143312</td>\n      <td>0.518269</td>\n      <td>-0.065130</td>\n      <td>-0.205698</td>\n      <td>...</td>\n      <td>-0.146927</td>\n      <td>-0.038212</td>\n      <td>-0.214048</td>\n      <td>-1.893131</td>\n      <td>1.003963</td>\n      <td>-0.515950</td>\n      <td>-0.165316</td>\n      <td>0.048424</td>\n      <td>5384.44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-0.206820</td>\n      <td>-0.165280</td>\n      <td>1.527053</td>\n      <td>-0.448293</td>\n      <td>0.106125</td>\n      <td>0.530549</td>\n      <td>0.658849</td>\n      <td>-0.212660</td>\n      <td>1.049921</td>\n      <td>...</td>\n      <td>-0.106984</td>\n      <td>0.729727</td>\n      <td>-0.161666</td>\n      <td>0.312561</td>\n      <td>-0.414116</td>\n      <td>1.071126</td>\n      <td>0.023712</td>\n      <td>0.419117</td>\n      <td>14278.97</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#data-overview .cell execution_count=3}\n``` {.python .cell-code}\n# Print the shape of the DataFrame\nprint(\"Shape of the DataFrame:\", df.shape)\n\n\n# Print the data types of each column\n# print(\"Data types of the columns:\")\n# print(df.dtypes)\n\n\n# Use .info() to get a concise summary of the DataFrame\nprint(\"DataFrame Information:\")\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of the DataFrame: (568630, 31)\nDataFrame Information:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 568630 entries, 0 to 568629\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   id      568630 non-null  int64  \n 1   V1      568630 non-null  float64\n 2   V2      568630 non-null  float64\n 3   V3      568630 non-null  float64\n 4   V4      568630 non-null  float64\n 5   V5      568630 non-null  float64\n 6   V6      568630 non-null  float64\n 7   V7      568630 non-null  float64\n 8   V8      568630 non-null  float64\n 9   V9      568630 non-null  float64\n 10  V10     568630 non-null  float64\n 11  V11     568630 non-null  float64\n 12  V12     568630 non-null  float64\n 13  V13     568630 non-null  float64\n 14  V14     568630 non-null  float64\n 15  V15     568630 non-null  float64\n 16  V16     568630 non-null  float64\n 17  V17     568630 non-null  float64\n 18  V18     568630 non-null  float64\n 19  V19     568630 non-null  float64\n 20  V20     568630 non-null  float64\n 21  V21     568630 non-null  float64\n 22  V22     568630 non-null  float64\n 23  V23     568630 non-null  float64\n 24  V24     568630 non-null  float64\n 25  V25     568630 non-null  float64\n 26  V26     568630 non-null  float64\n 27  V27     568630 non-null  float64\n 28  V28     568630 non-null  float64\n 29  Amount  568630 non-null  float64\n 30  Class   568630 non-null  int64  \ndtypes: float64(29), int64(2)\nmemory usage: 134.5 MB\n```\n:::\n:::\n\n\n## Research Questions\n\nOur first research question focuses on identifying the best-performing anomaly detection algorithm for this specific dataset and understanding the reasons behind its performance. We are particularly interested in exploring the effectiveness of various machine learning models, including but not limited to Random Forest, XGBoost, and Artificial Neural Networks and comparing these against ensemble techniques like stacking. The stacked generalization will combine multiple models to see if a meta-classifier can improve fraud detection performance by learning from the predictions of base classifiers. This approach not only tests the individual strengths of various algorithms but also explores the synergy between them, potentially leading to a more accurate and reliable fraud detection system. The mlxtend library is adopted to develop the stacked generalization technique.\n\n## Analysis plan\n\n| Task Name | Assignee | Due | Summary |\n|----------|----------|----------|----------|\n| Exploratory Data Analysis | Sai & Nandhini | 04/07/2024 | Comparing the statistical distribution of the anonymized features, Exploring the relationship between the amount of transactions and fraudulent transactions |\n| Feature Selection and Engineering | Gowtham | 04/09/2024 | Performing PCA and selecting the number of PCAs, Exploring random forest feature importance |\n| Training the Base Learners | Deema | 04/14/2024 | Training one machine learning algorithm from each of the ensemble learning approaches (bagging, boosting, and stacking) along with artificial neural networks |\n| Hypertuning Base Learners | Roxana | 04/20/2024 | Hypertuning the base learners using grid search or random search |\n| Model Evaluation | Sai | 04/24/2024 | Evaluating models using categorical metrics, confusion metrics, and ROC curve |\n| Developing Stacked Generalization | Omid | 04/30/2024 | Selecting the metalearner and Testing the potential improvement upon the base learners |\n| Preparing the Final Report and Presentation | Nandhini | 04/05/2024 | Finalizing the results and practicing the oral presentation |\n\n",
    "supporting": [
      "proposal_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}