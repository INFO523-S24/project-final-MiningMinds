---
title: "Fraud Detection Using Ensemble Learning"
subtitle: "Team : Mining Minds -INFO 523- Spring 2023 - Project Final"
author: "Omid Zandi,Nandhini Anne, Sai Navya Reddy Busireddy, Gowtham Gopalkrishnan, Roxana Akbarsharifi, Deema Albluwi"
title-slide-attributes:
  data-background-image: images/title1.png
  data-background-size: stretch
  data-background-opacity: "0.2"
  data-slide-number: none
format:
  revealjs:
    theme:  ['data/customtheming.scss']
    background-transition: fade
    transition: slide
    auto-animate-duration: 1.5
    scrollable: true
    logo: images/credit_card_fraud.jpeg
    footer: "[ðŸ’µ MiningMinds](https://info523-s24.github.io/project-final-MiningMinds/)"
  
editor: visual
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
editor_options:
    chunk_output_type: console
---

```{python}
#| label: load-packages
#| include: false

# Load packages here
import pandas as pd
import seaborn as sns
import joblib


```

```{python}
#| label: setup
#| include: false
#| 
# Set up plot theme and figure resolution
sns.set_theme(style="whitegrid")
sns.set_context("notebook", font_scale=1.1)

import matplotlib.pyplot as plt
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['figure.figsize'] = (6, 6 * 0.618)
```


## Introduction {style="font-size: 0.7em;"}
::: incremental

-   The primary goal of our project is to enhance machine learning models accuracy in detecting fraudulent credit card transactions using an ensemble learning technique known as stacked generalization.

-   The motivation behind our project is to improve the detection of fraudulent transactions, which remains a significant challenge in financial security.

-   By integrating multiple predictive models, the project aims to create a robust system that can more accurately identify fraudulent transactions, thus contributing to safer financial environments.

-   Despite challenges like data imbalance and feature anonymization, we anticipate that stacked generalization will enhance fraud detection accuracy, demonstrating the effectiveness of ensemble methods in complex scenarios.

:::

## Dataset description {style="font-size: 0.7em;"}
::: incremental
-   The dataset comprises over 550,000 credit card transactions from European cardholders, collected in 2023.

-   It includes 31 features with transaction details such as amount and time, anonymized to ensure privacy and ethical compliance.

-   The anonymization of features presents challenges in interpreting the data, while the class imbalance poses difficulties in model training and accuracy.

:::



## Preview of the Dataset: First Few Transactions {style="font-size: 0.7em;"}




```{python}
#| label: Reading the dataset
import numpy as np
import pandas as pd
df = pd.read_csv('./data/creditcard.csv')
df.head()
```

## Exploratory Data Analysis
::: panel-tabset

### Kernel Distribution
![](images/kernel.png)

### PCA
![](images/pca.png)

:::




## Research Questions {style="font-size: 0.7em;"}
::: incremental

-   What is the comparative performance of anomaly detection algorithms, including Random Forest, XGBoost, KNN, for fraud detection in this specific dataset?

-   How does the stacked generalization technique, implemented with the mlxtend library, improve fraud detection performance by leveraging the synergy between base classifiers?

:::

## Question1: Analysis Plan {style="font-size: 0.7em;"}
::: incremental

-  <b>Model Training and Sampling Techniques:</b> Address dataset imbalance by oversampling the minority class and undersampling the majority. Split the data into training and testing sets, and train anomaly detection models including Random Forest, XGBoost, and KNN.

-  <b>Model Optimization:</b> Hypertune the models to optimize performance, ensuring the best possible settings for each algorithm.

-  <b>Performance Evaluation and Analysis:</b> Evaluate each model on the testing set using metrics like precision, recall, F1-score, and ROC area. Analyze performance differences to understand the impact of model complexity, feature importance, and dataset characteristics.

:::




## Question2: Analysis Plan {style="font-size: 0.7em;"}
::: incremental

-  <b>Stacked Generalization Setup:</b> Utilize the mlxtend library to implement stacked generalization. Train base models as specified, and split their output into training and testing sets for the meta-classifier.

-  <b>Meta-classifier Training:</b> Combine predictions from base classifiers using the stacking method, and train a meta-classifier on these combined predictions to enhance prediction accuracy.

-  <b>Evaluation and Analysis:</b> Assess the performance of the stacked model, compare it to the base models, and analyze the performance gains, focusing on factors like model diversity, ensemble principles, and dataset specifics.

:::


## Random Undersampling {style="font-size: 0.7em;"}
::: panel-tabset
### PlotA
![](images/comparision.png)

### PlotB
![](images/undersampling.png)

:::


## Classification Models {style="font-size: 0.7em;"}
::: panel-tabset

## RandomForest
![](images/s_rff.png)

## KNN
![](images/s_knn.png)

## XGBoost
![](images/s_xg.png)

## Stacked Model
![](images/s_stacked.png)

:::

## Metrics for Classification Models {style="font-size: 0.7em;"}
::: panel-tabset

### Metrics
 Metric | RF | KNN | XGBoost | Stacked |
|----------|----------|----------|----------|----------|
| Accuracy | 0.98 | 0.98 | 0.97 | 0.97 |
| Precision | 0.08 | 0.08 | 0.05 | 0.05 |
| Recall | 0.96 | 0.91 | 0.99 | 0.99 |
| F1-Score | 0.14 | 0.14 | 0.09 | 0.09 |

### Analysis

-  Overall, Random Forest demonstrates a slight edge, where all other models exhibit commendable performance in this classification task, underscoring their effectiveness in predictive modeling. 

:::



## ROC {style="font-size: 0.7em;"}
::: panel-tabset

## Plot
![](images/roc.png)

## Analysis

::: incremental
- <b>Excellent Performance:</b> The ROC curves for four classifiersâ€”Random Forest, K-Nearest Neighbors (KNN), XGBoost, and a Stacked Classifierâ€”indicate high performance, with AUC values all above 0.98. Random Forest and XGBoost display the best performance, both achieving an AUC of 0.989, demonstrating their high effectiveness in discriminating between classes. The Stacked Classifier, despite combining features of multiple models, performs slightly lower than the top individual models with an AUC of 0.98. This suggests that while all models are highly capable, Random Forest and XGBoost might be preferred due to their marginally superior performance.



:::

:::






## Evaluation using the Whole Dataset {style="font-size: 0.7em;"}
::: panel-tabset

### RandomForest
![](images/rff.png)

### KNN
![](images/knn.png)

### XGBoost
![](images/xg.png)

### Stacked Model
![](images/stacked.png)

:::

## Metrics for Evaluation using whole dataset {style="font-size: 0.7em;"}
::: panel-tabset

### Metrics

| Metric | RF | KNN | XGBoost | Stacked |
|----------|----------|----------|----------|----------|
| Accuracy | 0.98 | 0.98 | 0.97 | 0.97 |
| Precision | 0.08 | 0.08 | 0.05 | 0.05 |
| Recall | 0.96 | 0.91 | 0.99 | 0.99 |
| F1-Score | 0.14 | 0.14 | 0.09 | 0.09 |

### Analysis

-   Precision is notably low for all classifiers, ranging from 0.05 to 0.08. Precision measures the proportion of positive identifications that were actually correct. These low scores suggest that while the models are good at finding positive cases (fraudulent transactions, for example), a large proportion of these predictions are false positives.

:::


## Future Implications {style="font-size: 0.7em;"}
::: incremental

-   SMOTE stands for Synthetic Minority Over-sampling Technique, a statistical technique designed to balance your dataset by increasing the number of cases. Instead of merely duplicating examples, SMOTE generates synthetic samples from the minority classâ€”the class with fewer instances. This approach effectively addresses the overfitting issue that arises when examples from the minority class are simply replicated.

-   using a more diverse set of machine learning models combined with a more sophisticated meta-learner can lead to more accurate results.
:::

##
![](images/thankyou.jpg){style="width:100%;height:100%;"}